Suervey:

	A Brief History of Recommender Systems
		推荐模型和推荐系统框架
		2005年之前：协同过滤在rs中占据主要作用
		2006-2009：矩阵分解
		2010-2016：lr、FM、FFM; 开始增加side-info
		2016-：
			特征交叉：wide&deep/deepFM/
			特征高阶交叉：dcn/dcn2/pnn/nfm/afm
			行为序列：din/dien/
			gnn-based：	node embed/ gnn
			drl-based:dqn
			因果推断-based


	A Survey on Neural Recommendation: From Collaborative Filtering to Content and Context Enriched Recommendation
	推荐类型：
		1 协同过滤； 问题：数据稀疏	 
			1 表示性学习
			2 给定表示时，user_item的交叉建模
		2 content-based;user或者item的side info
		3 context-based; 时空空间、历史行为等



RS：
	
	Explore, Exploit, and Explain: Personalizing Explainable Recommendations with Bandits  EE+解释性
		场景：将ee机制与解释性联合建模。 不同人对解释性的反应不同，同一个人不同上下文需求的可解释性推荐也不同。
		应用：推荐系统的排序问题？
		高德的特殊性： 地理位置导致的bias，召回受限于地理位置

	Graph Neural Networks in Recommender Systems: A Survey
		场景：rs的任务的核心是通过交互信息（cf）和side info 学习有效的user/item表示。 而GNN天然契合这个场景
		动机：从交互数据的利用上来说， 早期的做法主要是单挑信息，即user-item的直接信息。 而gnn可以模拟多跳；gnn能同时考虑user-user,item-item.user-item的信息。
		核心贡献：gnn在rs应用的类别体系，每个体系下的优缺点,未来的方向； graph构造 graph的表示学习





召回：
	Neural Collaborative Filtering  有些过时
		动机：DL只用在了建模辅助信息熵，而协同过滤的核心：user和item的召回仍然是是哦那个矩阵分解/内积等；本文提出NCF，将内积操作替换成concat+MLP
		核心工作：探索如何利用dnn来学习user和item的交叉关系。
		




ctr
	Deep Learning for Click-Through Rate Estimation
		动机：ctr综述，讲述从浅层模型到深层模型的进展。着重特征交叉/用户行为序列/automl等
		ctr应用场景：推荐/广告/
		模型：lr / poly /fm / wdl/ deepfm/ dien/ autoFIS/ amer/ubr/sim/rim； 从特征交叉，到用户行为序列
		NN模型的基本范式：
			embed&MLP； 问题：single dnn 学习高阶交叉特征很困难-》特征工程/special layer（prodict/cnn/attn）
			单塔/双塔：单塔模型的低阶交叉信号容易消失；  双塔明确有人工dot-feat和 nn的高阶特征，效果更好
			用户行为历史：item id和 特征构成行为 序列； 三种方法：attn/memory-based/retrieval-based
			automl：embed维度的自动设计/自动特征选择，特征组合/自动网络设计


	A Peep into the Future: Adversarial Future Encoding in Recommendation
		动机：推荐系统除了要充分利用历史设计，future information信息也很重要，future 信息和历史信息形成互补。但是future信息无法直接获得，本博文提出对抗future encoding框架来提取future信息
		方法：参考GAN，采用生成器和判别器的方式，使得生成器产生future information;  
			判别器：输入真实历史信息和future信息，判断是否合法
			生成器：根据历史行为，生成future information: 对item的点击/非点击等行为等所有依赖用户反馈的信息
			训练部分：参考irgan，使用强化学习进行训练

			在线应用：1 重排阶段的强化学习； 2 精排的point-wise

	用户行为序列
	[BST] Behavior Sequence Transformer for E-commerce Recommendation in Alibaba
		动机：embed&mlp的 ctr框架，忽略了重要信号：用户的行为序列，如点击序列。wdl 未考虑行为序列；din 没考虑行为的序。
		方法：利用bst：behavior sequence transformer,来提取行为序列信息，而后给mlp。序列中item的表示：id，类别id,位置特征；位置特征用时间差异表示
		实验：离线auc，在线收益

多任务:  
	Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations： 任务之间的强弱关联导致跷跷板效应如何处理
		动机：解决多任务的跷跷板效应
		背景：工业中尝试用MTL做推荐，通过不同任务之间的共享，提升学习效率。但是任务之间的弱关联、或者复杂关联导致多任务之间存在跷跷板效应； MMOE业界常用，但是这个模型也存在跷跷板效应
		模型：设计shared的expert和 task特定的expert，减轻expert之间的有害影响。
		创新点：分离task特定 expert和 shared expert; +多层结构； 明确share expert 和task expert； task= task expert+shared expert； 同时结合gate机制进行输入的re-weighting(动态weighting)
	
	ESMM   层次样本bias问题的处理
		背景：cvr的两个问题：ssb。样本偏执：“未点击!=未成交”； 数据稀疏问题：
		方式：通过ctr和 pctvr的方式建立loss， cvr作为模型的副产物，没有对应的target和loss。
		


re-rank
	Neural Re-ranking in Multi-stage Recommender Systems: A Review
		场景：推荐系统rerank综述。rerank:item对用户的西印度，不仅取决于自身，还取决于list-wise的其他item；rerank的优化目标：精度/多样性/公平性/
		rerank历程：最早只有MMR。
		rerank简介：输入是所有item，需要builld一个多元score。相比于rank只需要考虑一元score；nn rerank的两种架构：1 listwise context建模，通过历史行为数据；2 生成+评估的方式。
	
		训练数据来源：
			1 直接根据历史行为数据，进行list-wise建模；rnn-based（DLCM）； attention-based（PRM）
			2 假设item的相关性与排列顺序有关，即使相关的item，排列不同，其相关性也不同。； 生成+判别
		loss_func：
			1 point-wise:cross-entroy
			2 pair-wise: BPR loss/hinge loss
			3 Listwisw: kl-loss/
		多指标优化：
			多样性：非学习型：MMR; 又是一大堆模型；
			公平性：暂时应该不需要考虑
		模型比较：
			PRM>setrank>DLCM
		下一步方向：
			稀疏问题；
			多目标优化问题：精度/多样性/公平性；
			
	[DLCM] Learning a Deep Listwise Context Model for Ranking
		场景：nn rerank问题。从top-k的item中学习特征，来强化上一步的rank结果。
		动机： local ranking context（伪相关性反馈的一种）：,a well-studied framework is to represent each query with the top retrieved documents。 这篇论文想直接吧local ranking context建模到模型中。 根据topn的结果，来提取出query-specific的特征，对结果进行rerank：尝试根据topn的召回结果，分析出类似权威性/时效性/等跟比较依赖query的“需求度”指标。
		方法：精排结果的topn,给rnn， rnn的隐含state和输出用来re-rank精排topn结果； 同时加入基于attenion的list-wise loss函数。 基于local ranking context假设来做的排序。 基于attenion的listloss：分别计算label和模型输出的结果的attention系数（好像就是一个softmax???）；举的例子倒是挺有说服力。  does not directly predict the relevance labels of documents but focuses on the relative importance of each result in the ranked list
	
	
	[PRM]Personalized Re-ranking for Recommendation
		场景：上下文感知的rerank排序。推荐业务
		动机：典型的rank只考虑point-wide的<user,item>特征，而没有考虑list中的其他item；同时引入个性化部分，来表示用户的偏好和意图，结合list共同作用来排序。
		问题：re-rank需要考虑的：list中term之间的互信息/用户与list的相互作用（相同的list对不同人有不同的反应，也就是说互信息也要考虑个性化）。
		方法：个性化部分：用户对item的个性化embed（预训练得到）； item之间的信息：transformer; 输出：softmax
	
	Multi-Level Interaction Reranking with User Behavior History
		场景：上下文感知的rerank排序。推荐业务
		动机：rerank阶段对用户历史行为的利用还不够（只是把行为历史作为embed放进来）；现有的rerank主要关注item-level的关系，忽视了item的细粒度特征的交互（行为历史中的item的细粒度特征和target item的细粒度特征的交互）；用rank后的结果可能是次优的（把输入当集合，而非有序list）。
		方法：待排序list:处理成无序集合，用户行为:有序list。 初始集合和行为list分别做做self-att和bi-lstm处理；embed-layer； cross-item layer；set2list-layer（target item与用户行为序列）；



	Real-time Short Video Recommendation on Mobile Devices 
		端上实时重排
		业务背景：短视频，实时性反馈高。需要更准确的预测用户需求。目前的推荐都是用翻页请求获取结果，存在两个问题：1 只有新请求来时，后端才能及时响应；2 用户的实时反馈没办法充分利用。
		解决方式：将轻量级的模型部署到model上，提供实时排序能力，克服上述问题。端上实时重排
		核心点：实时信号的特征工程；实时上下文重排
		相关工作：edgerec
		server侧：负责长期兴趣、edge侧负责即是兴趣；
		特征工程：feature_diff 挺合理的看起来
		模型结构：multiheadattention+ mmoe + beam search寻找最优序列

	Globally Optimized Mutual Influence Aware Ranking in E-Commerce Search



多样性： 
	Managing Diversity in Airbnb Search


	Practical Diversified Recommendations on YouTube with Determinantal Point Processes
		背景：推荐系统里，精排产出的结果同质性太高，需要引入多样性。

EE： ---该复习DRL了
	DRN: A Deep Reinforcement Learning Framework for News Recommendation
		背景：在线推荐存在三个问题：1 只对当前收益建模（ctr）; 2 很少考虑点击/未点击外的其他反馈信号（用户使用频次）；3 重复推荐形似item，造成用户的反感；本文提出基于DQN的推荐框架，明确对未来收益建模；
		核心工作：利用DQN建模用户偏好和item的动态变化；提出新的探索策略，避免推荐不相关item。;考虑用户活跃度作为除了点击之外的反馈信号。
		方法：
			RL基本问题：mab和mdp。 与mab相比， mdp不仅可以建模当前的reward,还能考虑future的潜在reward.
			使用用户的连续状态特征表示和连续动作特征表示，作为输入到多层DQN，来预测潜在reward(click);
			模型框架：首先进行离线训练，在线时 agent与用户进行交互，然后更新网络；每个timestamp进行小更新，每个一段大的timestamp进行大更新一次；
			特征：item的one-hot特征；user:用户点击特征； usr-item交叉特征； 上下文特征（日期、新鲜度等）
			探索：强化学习中最常用的探索策略是贪婪和 ucb、汤普森采样； 本文提出，新建一个网络，在原来网络的基础上加上扰动，两个模型分别生成list，在线做interleave，加入探索网络的效果好，则用探索网络的参数去更新原来的网络。


特征处理
	An Embedding Learning Framework for Numerical Features in CTR Prediction
		场景：推荐系统中连续特征的embed学习框架
		动机：工作推荐系统中，ctr预估的标准范式是 embedding+mlp

学术探讨
	Neural Collaborative Filtering vs. Matrix Factorization Revisited  
		背景：embed逐渐成为协同过滤的主流；最近的NCF（concat(user,item)+MLP）用MLP来代替直接内积操作等。文章通过实验证明MLP的一些缺陷，也许直接内积效果会更好。NCF使用MLP来代替之前的内积
		核心：通过实验证明，一个仔细设计的内积 会超过MLP； 探索内积超过MLP的原因；MLP需要大量的数据来达到好的效果；内积对应矩阵分解；
		结论：除非数据量非常大，或者embed维度比较小（64一下），否则内积要好于MLP；要谨慎使用MLP代替内积的操作，MLP可能不适合作为相似度测量， （推荐系统是：多个异构输入源（user/item/context）的相似性）
		NCF： concat后的mlp 代替 向量内积
		原因分析：MLP虽然理论上的近似器，但是考虑到目标函数复杂时，所需要的参数空间会变大，这回导致学习到这个的函数会很困难。
		分类任务的最有一层也可以看作是内积。 NLP 也是遍布内积。   只依赖 user 和item embedding时，MLP学习内积相似性很困难，；
		！！！！DeepFM的FM部分也是在利用内积


long-tail 
	Empowering Long-tail Item Recommendation through Cross Decoupling Network；  （感觉是在模型层对不同热度的item或者user进行分层学习； user侧的意义大么？）
	背景: 工业推荐场景经常会遇到长尾问题：少量item的用户反馈占据大部分；其他item的反馈数据很少；
	动机: 从cv领域得到的启发：表示学习和分类学习所需要的数据分布是不一样的。cv中可以现在长尾数据上进行表示性学习，然后在重新采样的数据上进行分类学习；
	相关工作：
	核心工作：
		1 从item和user角度，提出了长尾影响推荐性能的理论分析； 
		2 item侧：把item的表述学习分成记忆部分和泛化部分；采用基于频次的gate网络，结合moe对记忆部分和泛化部分进行动态路由
		3 user侧：两个分支，主分支在原始数据上学习高质量的表示；另外一个分支在加入更多长尾信息的re-blanced分布上学习；
		
	核心方法:
		1 特征处理：特征分为记忆类（ID类，）和泛化类（tag属性，人工交叉类）；
			记忆类：通常是id类，embed形式，互相无干扰；
			泛化类：通常时用户或者item的偏好属性，在item之间share；  embed或者连续特征。
		2 item侧 设计memory和泛化的expert，基于频次的gate避免不同热度的item表示互相干扰；
		3 user 样本侧（cv侧借鉴的idea）； 两个分支网络，主网络包含所有用户反馈数据； 分支网络 包含所有长尾item的反馈，同时对热门反馈进行降采样； 推理阶段，只用main网络。（训练和测试采用不同的结构。）
		4 adapter层：根据学习的epoch，逐步提升长尾样本的权重。
			
			
	数据集：
	效果:
	消融实验:
	结论:
