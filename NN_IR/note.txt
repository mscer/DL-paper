Book on Pretrained Transformers for Text Ranking: BERT and Beyond
综述：两方便的应用：rerank（分类loss）和 学习稠密向量表示
	


Neural Ranking Models with Multiple Document Fields；   multifield+mask+dropout;
	背景:  nn经常只用在单域上，但是doc天然是多域的； bm25f就结合了多域的文档结构；本文探索nn在多域上的用法； 引入mask机制 处理field缺失；引入dropout方式某个域特征过强。  多域的联合建模好于多个filed结果的均值等。 多域的优势：考虑了文档结构；mask处理特征缺失，和变长； dropout防止局部特征过强。
	动机:
		doc的多个域之间包含互补的信息，如title短小精炼；body长，信息丰富等；文章提出nrm-f,学习doc的多域表示。
		doc的信息：doc自身信息； doc之间信息，anchor text; 用户行为信息；user-doc(clicked query可以表示doc)。 后两个可以叫权威性。
		一些研究表明：域之间可以提供互补的信息； 但是最新的doc或者低频doc 权威性特征会不足，用户点的少，或者缺少anchor,需要依赖相关性。
		多域的难点：每个field有自己的特性，body长、query短， click query可能是其他语种； 每个filed内包含多个instance（sections）;
	核心工作：
		多域处理结构；field-wise mask； field-wise dropout; 为啥不加个gate？  title/body/url/anchor/clicked query
		核心问题1：多域是否能提高排序效果；
		核心问题2：是否能超过bm25f和ltr
		核心问题3：多域联合建模是否比单域后summing更好
		核心问题4：分域的query表示是否好于单域query表示；
		核心问题5： field-wise mask 和 field-dropout的消融实验
	相关工作：
		在bm25基础上的bm25F模型；  并且联合多域建模的效果好比单filed然后聚合的方法好。
		deepmatch:; dssm;c-dssm; 早期交互、后期交互；
	核心方法:
		采用后交互的方法：query和doc分别去学习。doc内部field分别去学习，形成层级结构。
		doc:每个field内部参数共享，field内部instance分别学习。 field进行汇聚，sum/max/mean/concat+mlp;  field之间进行经过mlp 汇聚成doc表示。
		instance-wise表示：
		instance-wise(field内部)聚合： 每个field内部固定数量的instance，缺少的补0；concat+ mlp
		field之间聚合：concat+mlp
		*field-wise mask: padding代表缺失值，但是模型为认为是0，会进行梯度更新（gate?mask?）。  mask矩阵乘以instance的输出，mask掉instance的梯度。
		*field-wise dropout: 高区分性特征容易作用过强，导致其他特征作用不符合预期。需要field-wise 的dropout,  随机dropout整个field；
		*query【query的多域学习】: 结果与instance的表示类似，但是输出维度是 field-wise表示的和，从而实现不同field，有不同的query表示。
		match层：先计算哈达玛积，然后进行mlp； 没有直接进行内积或者cos，这两个操作会将field退话成float值，会影响field之间的区分性。
		训练：pairwise训练方式，loss中，用类似ndcg的方式来设置label（有点想lambdamart的trick）。
	基线：
		单域效果；
		bm25/bbm25F
		人工特征+ltr; lambdamart
		dssm、c-dssm
	数据集：
		bing语料；
		评估指标：ndcg
	效果:
		问题1：多域明显超过单域效果； 单域内部 clicked query效果最好。
		问题2： 相比bm25/ltr/dssm/cdssm/, 单域和多域的效果都更好； 
		问题3： 多域直接融合，超过了单域 汇聚或者加权和的方式
		问题4： query的多个表示效果，比1个表示效果好；
		问题5： 集合mask和 dropout效果更好，否则弱与LTR
	消融实验:
		1 分析学习曲线，判断需要多少训练预料。
	结论:
		提出用于doc多域表示的方法，引入field-wise mask处理特征缺失问题，field-wise dropout处理特征过强问题。


Ranker

Passage Re-ranking with BERT
	背景: bert在qa nli上取得了很好的效果，本文将bert应用到passage rerank上。
	动机: 大规模数据集的给出ms macro passage ranking，以及 bert 广义目标的语言模型。探索ir上的效果
	核心工作：
	相关工作： pre-bert的nn模型：drmm/knrm/；  qa包含三部分：召回，排序。生成。 ir这主要处理前两部分。
	核心方法: cross encoder  cls 过单层mlp， sigmoid函数
	基线： bm25/ knrm/;  bert远高于其他
	数据集：
	效果:
	消融实验:
	结论:
Document Ranking with a Pretrained Sequence-to-Sequence Model ——T5是否见过这些预料？
	背景:  现有的rerank都是只用encoder，本文 讲怎么用encoder-decoder来做rank， 并且encoder-decoder的效果好于encoder-only
	动机:
	核心工作：1 一种新的使用encoder-decoder的排序模型；2 消融和对比实验
	相关工作： bm25召回， cross encoder排序
	核心方法: 
		1 构造模板：Query: [Q] Document: [D] Relevant:； query替代[Q],doc 替代[D]。 decode生成true或者false;  相关性= true和false两个token的softmax归一化权重。 本文用了t5可以用到其他模型上，比如bart、unilm等
		2 在ms macro上训练，其他ir测试集上测试。 测试zero-shot能力。
	基线： bm25 /bm25+bert large
	数据集： ms macro上与基线模型对比。 其他几个用下验证zero-shot能力。
	效果:  t5 在macro上超过sota;  在zero-shot上 t5-3b效果也好于之前的ft过的bert; 
	消融实验: 
		参数量的影响：
			“T5-base 超过bert-large（参数量更大）”；  但是t5-3b的收益是否来源于参数量的提升，不好对比。
		训练集大小的影响：
			1 训练集增加是，bert和t5效果都会变好；
			2 数据量少时，T5也好于bert
			3 zero-shot时，原始数据上最好的 在新数据上不一定最好，可能会存在overtrain问题。It makes sense that fine-tuning more and more on a specific dataset would reduce the model’s ability to generalize to other domains.
			4 如何选择排序的准则：只用true的概率？ 所有token上softmax，还是 只在true或者false上softmax,实验证明true和false上softmax效果更好。 1 可不取，2 和3 差异不大。
			5 为何t5效果更好：
				1 bert上， cls上的一层mlp需要完全的重新训练。 只利用了encoder的知识
				2 t5上， decoder的输出本身已经含有了语义信息（参考输入template），即T5decoder侧具备额外的能力。
				3 进一步验证实验，尝试用其他token（颠倒true/false）模拟t5的 语义mapping，大批量数据时，与true/false一致。但是小流量数据时，由于语义的不一致， t5效果更差。
		
	结论:
		1 nn-based rank模型仍需要大量数据的训练才能与ltr match；但是 transformers-based的模型能超过ltr
		2 encoder-decoder的模型在排序上超过encoder-only，尤其是在data-poor场景下。
		3 验证encoder-decoder好的原因，是因为decoder部分具有的本身要输出流畅的text能力，能帮助提升效果。———— 生成的方式将下游任务与预训练统一了，而encoder-only的模型是全新的ft



Investigating the Successes and Failures of BERT for Passage Re-Ranking 
	背景: bert在passage rank上取得了sota结果，本文要分析结果，来更好的理解取得sota的原因。利用ms marco来分析成功和失败的原因。
	动机: 使用question和passage表示的bert，就能在ir上拿到sota结果，通过分析bert的排序结果，找到bert效果好的原因
	核心工作：提出了bm25 和bert的几个假设，分别验证假设是否成立，以及分析bert执行好和不好的query类型
	相关工作： 预训练后的bert，额外加一层后进一步ft,在一些任务上都有了sota结果。mrc/nli等
	核心方法: 在ms Marco上ft过的bert模型。 
	基线：
	数据集： ms macro数据集 bing搜索的query和人工标注的是否相关。
	效果:  
		1 相比bm25 ,bert结果的tf会低一些。
		2 从统计上看，随着tf的上升，mrr效果会有下降。bm25
	消融实验:
		问题1 bert能不做语义相似性，但是 当对term的语义理解与想要的不一样时，效果会变差。尤其是query自身有歧义时。bert会更倾向于给出热门的结果，而且热门的远大于非热门的。
	结论:
		We showed that BM25 is more biased towards high query term frequency and this bias hurts its performance. We demonstrated that, as expected, BERT retrieves passages with more novel words


Learning-to-Rank with BERT in TF-Ranking
	背景:  
	动机:认为 cls+二分类的方式不适合排序任务，应该引入ltr
	核心工作： query+passage 拼接过cross encoder
	相关工作： 
	核心方法:  形成一对多的正负对，分别进行 point-wise/pair-wise/list-wise的实验（softmax）
	基线：
	数据集： ms macro 数据集； rerank实验（top1000rank）； full ranking (所有集合rank)实验；
	效果: softmax>pairwise>point wise;   与对比学习比呢？  Infonce infonce 假如负样本就是 query的负样本，那么就等价于sofmax的listwise; 如果是随机负采样， 则不一样。
	消融实验:
	结论:



How Different are Pre-trained Transformers for Text Ranking
	背景：分析cross_encoder 和bm25性能区别，分析相同点和不同点。bert cross_encoder在文本检索上的收益来源还没有被很好的理解。
	目标：
		1 分析bert和bm25的差异；分析bert是否把bm25召回的文档排序更好；bert是否召回了bm25漏召回的文章。
		2 定量分析精确匹配和soft匹配对总体性能的贡献：bert是否包含精确匹配，bert是否能找到看似不相关的相关结果。
	相关工作：
		之前的工作证明bert的优势在于精确匹配和term重要性。
	数据集：ms marco qa
	对比基线：bm25
	实验结论：
		1 精确的文本匹配在bert中是一个很重要的因素
		2 bert和bm25 各自存在一些效果不好的case.
		3 bert能补充召回bm25漏召回的55% 的case
		4 *假如doc只保留query中出现的term，bert效果会变差。bert并没有充分利用精确匹配信号。
		5 *加入doc中删除query中出现的 term，bert效果仍然还好，甚至好于只保留query term的指标。 这个地方是bert真正的优势，soft match
	bert存在的问题：精确匹配关注不足，核心优势soft 匹配。
		

Understanding the Behaviors of BERT in Ranking
	背景：通过passage reranking（qa的段落排序） 和 doc ranking（网页排序）任务来评估bert的性能。评估bert对语义匹配和文本匹配的侧重点，预计相比点击模型（nn训练）的优势。
	现状：在qa上效果好，但是doc ranking效果弱于基于特征的ltr和点击日志训练的nn模型。	
	对比基线：bm25/ranksvm/k-nrm/conv-knrm
	方法：bert双塔/query-doc拼接输入的cross_encoder的最有一层cls/ query-doc拼接cross_encoder的所有层cls/。训练时采用分类loss，尝试过pairwise loss但是效果上差异不大。
	数据集：ms marco passage rerank和 trec doc ranking
	效果评估：
		*在marco上：交互式bert远超过knrm相关。但是双塔效果很差，表明bert是一个交互式match model；
		*在trec ranking上：bert双塔和交互模型都弱于ltr和nn的点击模型。这表明 marco更像是seq2seq的任务，而doc排序相比之下需要更多的其他信号（bert的优势是surrounding text和target term的推断能力）（是否可以理解成，qa成内容是自包含的， 而doc排序中doc有很多跟query无关的term）
	实验分析：
		通过attention权重分析term的贡献
		通过with/with-out分词term的重要性。 相比conv-knrm，大部分的单term移除对bert影响不大，但是几个特殊term的移除，会导致bert的打分极具变化。
	结论：
		bert在网页上的排序与之前的观察一致，bert采用邻域文本训练的方式，更适合与语义匹配的任务；而在web doc上，如果不适用点击日志，效果会差一些
		bert是交互性的seq2seq的match model；而在web doc上，点击信号更重要。






Recall
Dense Text Retrieval based on Pretrained Language Models: A Survey； 
  	总结：语义召回和term召回是互补的， 因为语义存在语义压缩现象，而term召回会考虑term的命中与否。——问题 ： term引入term weighing是否会退化成语义？？； 向量表示能否变长？；dense models are easier to be affected by adding non-relevant contents
	背景: 信息检索逐步发展为有效的表示文本，以及建模相关性match上； 另外 plm的zero-shot能力比较差时，面对新query效果会弱与term；
	核心工作： 
		1 重点介绍plm-based ir；
		2 深度向量召回的实用技术；
			模型结构、训练方法、index机制、召回pipeline
		检索类型：doc/passage/sentence 
	相关工作：
		bow-> NIR(pre-bert)->  plm-based ir
		系数召回：term weighting/ 紧密度、term扩展
	核心方法: 
		负样本需要通过采样挖掘得到

		开源数据集和评估指标：  doc、passage
			ms macro;
			dureader-retrieval;
		评估指标	
			map、mrr、ndcg
		开源代码库：
			sentence-bert
			openmatch

	核心内容：
		模型结构
			基础：transfromer; pretrain+ft
			bi-encoder： colbert
			cross-encoder: 使用cls或者 token向量均值。
			bi-encoder and cross-encoder: bi做召回，ce 做ranker；  ce给bi构造训练预料等。
			与稀疏召回对比：
				dense召回擅长于解决语义问题：  压缩到特定长度，不可避免的存在忽视部分语义的情况。
				稀疏召回商场与精确匹配问题。
			稀疏召回和稠密召回各有优势，可以互补。 稠密无法代替系数原因有几种分析，仍然输出待探索问题。

		模型训练
			1 loss函数：-log-likehood(softamax后的概率分布，正样本越大越好，相对的好坏))；  像对比学习的loss(infonce); triple loss;rank侧则使用bce loss 二分类判定是否相关。（绝对的相似）； 其他考虑doc-doc的相似性作为分母。
			2 相似性度量：内积、cos、欧式距离。 主要用cos和内积； cos和内积的区别：cos会进行归一化，内积天然有值的绝对序。
			3 负样本选择： false negative样本的处理；  随机负样本+hard选择+ false negative
				1 in-batch负； 类似对比学习， 扩充batch-size,提升训练效果。 问题：不能得到足够的有效负样本；
				2 cross-batch负；扩充负样本的量。
				3 hard负样本： 1和2 都是随机负样本，缺乏hard负样本。  
					1 static hard样本			
						高bm25的负
					2 动态hard样本
						采用训练的模型， 抽取高打分的负样本
					3 false negative 检测：
						向量召回对负样本质量很敏感。
						采用cross-encoder 进行false negative的检测。
			4 数据增强：
				1 引入其他辅助数据集
				2 大模型蒸馏
			5 深度召回模型的预训练
				仍然采用bert的预训练方式，但是模拟检索过程。
				1 任务自适应pretrain:
					ICT: 
				2 生成增强pretrain:
					使用bart生成query和doc
				3 召回增强预训练。  增强 召回能力
					realm/rag
				4 表示增强预训练：   增强cls的语义表示能力。
					cls在预训练时，并不作为sentence表示； 加入decoder 让cls生成原始输入，提升cls的能力；但是要避免decoder陷阱（当decoder过分强大时，decoder自身依赖上文即可完成后续的生成）
					对比学习增强
					
		向量建库
			稀疏召回：es
			稠密召回：
			anns:近似最近搜索；
			量化：

		集成到系统
			基于bi-encoder的召回
			基于cross-encoder的rank
			召回-rank pipeline的训练：
				1 分开并行训练；
				2 自适应训练：两个模块交替训练。

		
	基线：
		bm25  dpr
	数据集：
	效果:
	消融实验:
	结论:

Que2Search: Fast and Accurate Query and Document Understanding for Search at Facebook
	背景:  使用xlm或者xlm-roberta, 结合多任务、多模态。的sentence表示模型，在离线和在线业务上效果提升。在线rt<1.5 ms 纯cpu调用
	动机: ads
	核心工作： 限制：属性信息有噪声、跨语言问题、多模态问题、rt限制。
	相关工作： facebook之前的向量召回模型还是ebr， text feature+location feat+social embed； 底座是nn； 这里换成bert
	核心方法: 
		模型：query和product分别用两层和三层的xlm表示。训练融合了2阶段课程学习、多模态处理（text和image）、多任务学习(分类loss)等，同时优化query-to-product的召回任务和product的分类任务； 使用ml可解释技术来理解模型，找到优化点。
			query表示： country embed/text encoder的融合attention
			doc表示： title+描述的text encoder、图片的embed , bag of embed of 3gram 融合attention
		训练：
			训练数据： in-batch负样本。 infonce? 只需要挖掘高质量的正样本
		课程学习：
			设计课程学习框架， 逐步feed harder样本，auc提升1%
			hard样本：in-batch内打分最高的负样本，认为是hard负。  刚开始课程学习效果并不好，经过分析发现需要确保第一轮训练收敛；
		评估：
			
		应用：
			There is no silver bullet for retrieval
			召回侧，que2search与其他召回方式并行，比如term-based召回、位置召回、social召回等。即只把向量召回作为解决语义匹配的方法。原因：1 性能因素；2 向量召回缺乏硬性fiter；3 term召回的高效性。
	
	基线：
	数据集：
	效果:
	消融实验:
	结论:

点击模型：
	通过挖掘点击数据，来计算query的真实需求，计算query-doc的相关性。现实条件往往受bias等因素影响，导致点击数据不能完全可信。
	PBM:基于位置的模型
	CM:基于瀑布流的模型，点击跳过首次点击

		


IR；  召回和排序两类：
	

	召回的基线：bm25和 平滑的smt
	排序的基线：各种ltr

	term weighing部分:
		基线：tf*idf;textrank; 基于回归的模型
		NN: DeepTR  deepCT  HDCT
	

		
	Global Weighted Self-Attention Network for Web Search
		方法：把bm25的统计term weighting，结合到dssm中。 多域doc
		应用：nn召回； 双塔
	
	Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval 
		应用：召回； 计算term-weighting 跟bm25结合 
		bert+回归layer计算term weighting。但是要拟合的目标需要build；能否直接依赖搜索日志，直接训练，减少label挖掘哪一步？
	
	
	Context-Aware Document Term Weighting for Ad-Hoc Search 
		应用：召回； 计算term-weighting 跟bm25结合 
		方法：deepCT扩展到doc粒度，缓解长度限制；引入label的自动生成策略

	vpcg:Learning Query and Document Relevance from a Web-scale Click Graph 
		应用：排序特征；
		方法：点击二部图，训练query和doc的表示。 bow表示，学到的是term的权重系数

	dssm
		应用：nn排序； 双塔
		数据：点击样本为正样本，未点击样本为负样本，1:4
	cdssm
		应用：nn召回； 双塔

	arc1 arc2(李航的)
		应用：nn排序； interaction-based
	

	de-bert:dual encoder bert
		应用：nn排序； 双塔
	
	drmm
		应用：nn排序
	k-nrm : softmatch
		方法：计算query与doc的term的相似性， 引入kernel来抽取相似性特征。kennel是常见pooling（mean,max）的泛化。   更好的poolling ,有点意思：除了mean,max之外的可导poolling。 不过kernel参数是否也能做成可导的？
		应用：nn 排序；  交叉表示
	Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search 
		方法：
		应用：
	
	Sparse, Dense, and Attentional Representations for Text Retrieval 
		方法：
		应用：
	
	