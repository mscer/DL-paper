

Layer Normalization
	背景:normalization可以提升训练效率，减少训练时间；bn减少了训练时间，但是bn不适用于rnn。rnn长度不定，bn无法直接使用
	动机:ln不依赖batch_size.
	核心工作
	核心方法:
		在每一layer上进行normalization.与batch_size无关；
		rnn中的ln: scale和shift在time step中共享。 均值和方差每个time step分别计算。
		每个step ln后 ，整体分布一致，更方便rnn的训练。
	效果:
	消融实验:
	结论:

How Does Batch Normalization Help Optimization?  bn起作用的真正原因-》更平滑的优化
	背景: 分析BN起作用的核心原因:使优化更加平滑；BN：通过使layer input更加稳定来提升NN的训练效果；但是我们对BN起作用的原因了解的仍然不是很清楚。
	动机:
		研究bn有效的原因。
	核心工作：
		1 证明bn与internal covarate shift无关，或者没有直接联系
		2  证明bn 有效的原因：使得优化显著显著平滑。从而使得可以用更大的学习率和更快的收敛速度。
		3 不仅是BN,其他的normalization技术也有相似的作用。
	核心方法:
		1 通过绘制有无bn 模型train和test的性能，证明bn的作用；
		2 通过比较有无bn 相同layer的分布，证明与internal covarate shift关联性不大； 增加噪声来模拟icl； 
		3 实验的设计：比较同一层在前序层梯度更新与否的梯度差异。按照之前ics的理论，加入bn后，梯度差异会更小，但是实验发现加入bn后反而梯度差异更大了。
		4 BN work的原因：使得优化更加平滑，传统的dnn优化存在局部最优问题，使得梯度下降不稳定，梯度消失或者膨胀，从而对初始化和学习率选择敏感
		5 bn是否最有效：实验证明，其他形式的norm(1,2,p)效果也不差，甚至好于bn。
		
	效果:
	消融实验:
	结论: bn 有效的原因与icl关系不大，而是因为bn使得优化问题更加平滑，缓解了对学习率和初始化依赖，减轻梯度消失和梯度爆炸，可以用更大的学习了，提升训练稳定性


Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction； ctr任务上不同部分的layern norm的选择
	背景:normalization是nn中的一项重要对技术，nn中mlp部分经常使用normalizatin，但是这一技术没有被系统研究过。1 normalization的种类（bn/ln/gn/in）；2 对不同层的影响
	动机: 各种各种的normalization对ctr的影响是啥，是否存在最适合ctr任务的normlization，比如bn之于cv；ln之于nlp;  normalization work的原因是什么; 
	核心工作：	
		提出一种新的normalization方法；
			
		大量实验，验证不同normalization的影响。
		发现影响normalization作用的核心因素
	
	核心方法:
		simple ln:去掉bias和scale,参数。
		VO-ln:去掉bias scale 去掉re-center;
		normalization的位置：embed上、mlp上、 embed和mlp上
		embed上的normalization:
		VO-ln normalization作用的原因：
			把nn的输出的得分降低，结合rule的激活函数，相当于去除了部分网络的输出。减少了噪声的影响，提升了训练速度。
			
		数值特征适用的normalization:
			数值特征embed后，用LN最合适。
		embed特征适用的normalizatin:
			取决于具体任务，有时bn好，有时ln好
		mlp适用的normalization:
			ln比较好，simple ln或者VO-ln 效果更好。
	效果:
	消融实验:
	结论:。 选择合适的BN，dnn甚至能超过XdeepFM
 
On Layer Normalization in the Transformer Architecture (preln和postln的比较; )
	mynote:如果从泰勒展开来看，preln是在宽带的延展，类似reset，每一层需要学习的内容随层数加深而减少，梯度自然减少； 而postln 则是深度的扩展，每一岑需要的信息量时全新的，这一点也与文章的论点一致，但是效果上应该是postln更好
	
	背景: transformer的训练，需要引入学习率的warm-up阶段，这一阶段对最终的性能很重要，但是warm-up会减慢优化过程，并且带来了更多的超参数需要调节。 提出了mean filed 理论，证明1 训练初始阶段，postln的期望梯度在输出层会更大，如果直接使用大学习率会导致训练不稳定问题，引入warm-up可以缓解；其次如果采用preln,训练初始阶段的表现会更好，甚至可以去掉warm-up阶段。
	动机:能否找到一种方法，去掉warm-up, 减少对超参数的依赖。经过试验发现layey norm可以控制梯度值的大小，post-ln训练不稳定的原因是靠近输出侧梯度很大， 容易训练不稳定。那么能否找到ln合适的位置，在训练效果和训练稳定性上达到平衡。
	核心工作：
		研究preln和postln在初始训练阶段上，的梯度差异。
		证明用preln时，可以去掉warm-up策略。（只是去掉warm-up,但是学习率调整还需要）
		初始化阶段，preln则没有任何梯度爆炸梯度小时问题，可以去掉warm-up阶段。
		1 探索preln postln在初始阶段的grad变化，并证明为什么postln 必须要有warm-up
		2 证明可以去掉pre-ln的warm-up阶段。结合合适的学习率调整，训练时间可以大幅度减少。
	相关工作：
		1 对于cnn。rnn 一般是刚开始选择大的学习率，然后逐步减少学习率；只有在大batch时才会考虑用warm-up
		2 但是在优化transformer时，经常需要学习率的warm-up;  尤其是post-ln,不加 warm-up或者warm-up比较小时，会存在训练发散现象。 而preln甚至可以去掉warm-up

	核心方法:
		使用postln时，靠近输出层的梯度很大， 需要更小的学习率,帮助收敛; 否则梯度过大会存在训练不稳定问题；
		layer norm可以起到控制gradient值的作用。
		1 学习率warm-up阶段的实验现象

			在同一任务上，分别实验不同优化器adam和sgd,在不同warm-up设置下的效果。post-ln
			结论：1 对于两种优化器，warm-up都是必须的；warm-up的设置对学习过程影响大，warm-up越大效果越好
		2 理解transformer的参数初始化，经过一系列的简化，
			1 postln的梯度与层无关； 而preln的梯度会随层数加深，缓慢减小。
			2 实验验证：postln 和preln在不同层的梯度期望，符合假设。
		3 对preln的实验：
			1 对于preln来讲，warm-up不是那么重要。
			2 使用更大的学习率时，preln收敛更快。
	基线：
	数据集：
	效果:
	消融实验:
	结论:效果:
	消融实验:
	结论：post-ln在输出层的梯度过大，导致训练不稳定问题，必须借助于warm-up提高训练稳定性；而perln可以在不加入warm-up的情况下训练，并且训练收敛速度更快。